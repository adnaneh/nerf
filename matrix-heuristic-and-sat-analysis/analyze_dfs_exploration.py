#!/usr/bin/env python3
"""
DFS Exploration Analysis Script

Analyzes the CSV files generated by hireme2_exploration.c to extract insights
about the DFS search space and decision patterns.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import os
from pathlib import Path
import argparse
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

class DFSExplorationAnalyzer:
    def __init__(self, csv_pattern="dfs_exploration_round_*.csv"):
        """Initialize analyzer with CSV file pattern."""
        self.csv_pattern = csv_pattern
        self.data = {}
        self.combined_data = None
        self.choice_columns = []
        
    def load_data(self):
        """Load all CSV files matching the pattern."""
        csv_files = glob.glob(self.csv_pattern)
        if not csv_files:
            raise FileNotFoundError(f"No CSV files found matching pattern: {self.csv_pattern}")
        
        print(f"Found {len(csv_files)} CSV files to analyze:")
        for file in sorted(csv_files):
            print(f"  - {file}")
        
        # Load each CSV file
        for csv_file in sorted(csv_files):
            round_num = self._extract_round_number(csv_file)
            try:
                df = pd.read_csv(csv_file)
                if df.empty:
                    print(f"Warning: {csv_file} is empty, skipping...")
                    continue
                    
                df['round'] = round_num
                df['file'] = csv_file
                self.data[round_num] = df
                print(f"  Loaded round {round_num}: {len(df)} nodes")
                
                # Identify choice columns from first file
                if not self.choice_columns:
                    self.choice_columns = [col for col in df.columns if col.startswith('choice_')]
                    print(f"  Choice columns identified: {self.choice_columns}")
                    
            except Exception as e:
                print(f"Error loading {csv_file}: {e}")
        
        if not self.data:
            raise ValueError("No valid data loaded from CSV files")
        
        # Combine all data
        self.combined_data = pd.concat(self.data.values(), ignore_index=True)
        print(f"\nCombined dataset: {len(self.combined_data)} total nodes across {len(self.data)} rounds")
        
    def _extract_round_number(self, filename):
        """Extract round number from filename."""
        import re
        match = re.search(r'round_(\d+)', filename)
        return int(match.group(1)) if match else 0
    
    def basic_statistics(self):
        """Generate basic statistics about the exploration."""
        print("\n" + "="*60)
        print("BASIC STATISTICS")
        print("="*60)
        
        total_nodes = len(self.combined_data)
        winning_nodes = len(self.combined_data[self.combined_data['on_winning_path'] == 1])
        dead_end_nodes = total_nodes - winning_nodes
        
        print(f"Total nodes explored: {total_nodes:,}")
        print(f"Nodes on winning paths: {winning_nodes:,} ({winning_nodes/total_nodes*100:.2f}%)")
        print(f"Dead end nodes: {dead_end_nodes:,} ({dead_end_nodes/total_nodes*100:.2f}%)")
        
        print(f"\nDepth statistics:")
        print(f"  Max depth: {self.combined_data['depth'].max()}")
        print(f"  Mean depth: {self.combined_data['depth'].mean():.2f}")
        print(f"  Median depth: {self.combined_data['depth'].median():.2f}")
        
        print(f"\nNext combinations statistics:")
        combinations = self.combined_data['next_combinations']
        print(f"  Max combinations: {combinations.max():,}")
        print(f"  Mean combinations: {combinations.mean():,.2f}")
        print(f"  Median combinations: {combinations.median():,.0f}")
        
        # Per-round statistics
        print(f"\nPer-round breakdown:")
        for round_num in sorted(self.data.keys()):
            df = self.data[round_num]
            winning = len(df[df['on_winning_path'] == 1])
            total = len(df)
            print(f"  Round {round_num}: {total:,} nodes, {winning:,} winning ({winning/total*100:.2f}%)")
    
    def analyze_choice_patterns(self):
        """Analyze patterns in the choices made."""
        print("\n" + "="*60)
        print("CHOICE PATTERN ANALYSIS")
        print("="*60)
        
        if not self.choice_columns:
            print("No choice columns found in data")
            return
        
        # Choice distribution analysis
        print(f"\nChoice distribution analysis for {len(self.choice_columns)} choice dimensions:")
        
        choice_stats = {}
        for col in self.choice_columns:
            values = self.combined_data[col].dropna()
            if len(values) > 0:
                choice_stats[col] = {
                    'unique_values': values.nunique(),
                    'max_choice': values.max(),
                    'mean_choice': values.mean(),
                    'most_common': values.mode().iloc[0] if len(values.mode()) > 0 else None
                }
                
                choice_value = col.replace('choice_', '')
                print(f"  {choice_value}: {values.nunique()} unique choices, "
                      f"max={values.max()}, mean={values.mean():.2f}, "
                      f"most_common={choice_stats[col]['most_common']}")
        
        # Winning vs losing choice patterns
        print(f"\nChoice patterns: Winning paths vs Dead ends")
        winning_data = self.combined_data[self.combined_data['on_winning_path'] == 1]
        losing_data = self.combined_data[self.combined_data['on_winning_path'] == 0]
        
        for col in self.choice_columns[:5]:  # Show first 5 to avoid clutter
            if col in winning_data.columns and col in losing_data.columns:
                win_mean = winning_data[col].mean()
                lose_mean = losing_data[col].mean()
                choice_value = col.replace('choice_', '')
                print(f"  {choice_value}: Winning paths avg={win_mean:.2f}, "
                      f"Dead ends avg={lose_mean:.2f}, diff={win_mean-lose_mean:.2f}")
    
    def analyze_winrate_per_choice(self):
        """Analyze winrate for each specific choice value in each choice column."""
        print("\n" + "="*60)
        print("WINRATE PER CHOICE ANALYSIS")
        print("="*60)
        
        if not self.choice_columns:
            print("No choice columns found in data")
            return
        
        print(f"\nDetailed winrate analysis for each choice in {len(self.choice_columns)} dimensions:")
        
        # Store all winrate data for export
        self.choice_winrate_data = {}
        
        for col in self.choice_columns:
            choice_value = col.replace('choice_', '')
            print(f"\n{choice_value} (column: {col}):")
            print("-" * 50)
            
            # Get data for this choice column (excluding NaN/missing values)
            choice_data = self.combined_data[self.combined_data[col].notna()].copy()
            
            if len(choice_data) == 0:
                print("  No data available for this choice")
                continue
            
            # Calculate winrate for each choice value
            winrate_stats = choice_data.groupby(col).agg({
                'on_winning_path': ['count', 'sum', 'mean']
            }).round(4)
            
            winrate_stats.columns = ['total_nodes', 'winning_nodes', 'winrate']
            winrate_stats = winrate_stats.sort_values('winrate', ascending=False)
            
            # Store for later export
            self.choice_winrate_data[choice_value] = winrate_stats.copy()
            
            print(f"  {'Choice':<8} {'Total':<8} {'Winning':<8} {'Winrate':<10} {'Percentage'}")
            print(f"  {'-'*8} {'-'*8} {'-'*8} {'-'*10} {'-'*10}")
            
            for choice_val, row in winrate_stats.iterrows():
                percentage = f"{row['winrate']*100:.2f}%"
                print(f"  {int(choice_val):<8} {int(row['total_nodes']):<8} {int(row['winning_nodes']):<8} "
                      f"{row['winrate']:<10.4f} {percentage}")
            
            # Additional statistics
            total_choices = len(winrate_stats)
            best_choice = winrate_stats.index[0] if len(winrate_stats) > 0 else None
            worst_choice = winrate_stats.index[-1] if len(winrate_stats) > 0 else None
            avg_winrate = winrate_stats['winrate'].mean()
            winrate_std = winrate_stats['winrate'].std()
            
            print(f"\n  Summary for {choice_value}:")
            print(f"    Total unique choices: {total_choices}")
            print(f"    Best choice: {int(best_choice) if best_choice is not None else 'N/A'} "
                  f"(winrate: {winrate_stats.loc[best_choice, 'winrate']:.4f})" if best_choice is not None else "")
            print(f"    Worst choice: {int(worst_choice) if worst_choice is not None else 'N/A'} "
                  f"(winrate: {winrate_stats.loc[worst_choice, 'winrate']:.4f})" if worst_choice is not None else "")
            print(f"    Average winrate: {avg_winrate:.4f}")
            print(f"    Winrate std dev: {winrate_std:.4f}")
            
            # Identify statistically significant choices (those with >50 samples)
            significant_choices = winrate_stats[winrate_stats['total_nodes'] >= 50]
            if len(significant_choices) > 0:
                print(f"    Statistically significant choices (≥50 samples): {len(significant_choices)}")
                if len(significant_choices) > 1:
                    best_significant = significant_choices.iloc[0]
                    print(f"      Best significant choice: {int(significant_choices.index[0])} "
                          f"(winrate: {best_significant['winrate']:.4f}, "
                          f"samples: {int(best_significant['total_nodes'])})")
    
    def export_choice_winrates(self, filename="choice_winrates.csv"):
        """Export detailed choice winrate data to CSV."""
        if not hasattr(self, 'choice_winrate_data'):
            print("No choice winrate data available. Run analyze_winrate_per_choice() first.")
            return
        
        print(f"\nExporting choice winrate data to {filename}...")
        
        # Prepare data for export
        export_data = []
        
        for choice_name, winrate_stats in self.choice_winrate_data.items():
            for choice_val, row in winrate_stats.iterrows():
                export_data.append({
                    'choice_dimension': choice_name,
                    'choice_value': int(choice_val),
                    'total_nodes': int(row['total_nodes']),
                    'winning_nodes': int(row['winning_nodes']),
                    'winrate': row['winrate'],
                    'percentage': f"{row['winrate']*100:.2f}%"
                })
        
        # Create DataFrame and export
        export_df = pd.DataFrame(export_data)
        export_df.to_csv(filename, index=False)
        
        print(f"Choice winrate data exported to: {filename}")
        print(f"Total records: {len(export_data)}")
    
    def generate_choice_winrate_plots(self, output_dir="analysis_plots"):
        """Generate visualizations for choice winrates."""
        if not hasattr(self, 'choice_winrate_data'):
            print("No choice winrate data available. Run analyze_winrate_per_choice() first.")
            return
        
        print(f"\nGenerating choice winrate visualizations...")
        Path(output_dir).mkdir(exist_ok=True)
        
        # Create subplots for each choice dimension
        n_choices = len(self.choice_winrate_data)
        if n_choices == 0:
            return
        
        # Calculate subplot layout
        n_cols = min(3, n_choices)
        n_rows = (n_choices + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
        if n_choices == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, (choice_name, winrate_stats) in enumerate(self.choice_winrate_data.items()):
            row = i // n_cols
            col = i % n_cols
            ax = axes[row, col] if n_rows > 1 else axes[col]
            
            # Filter for choices with at least 10 samples for cleaner visualization
            filtered_stats = winrate_stats[winrate_stats['total_nodes'] >= 10]
            
            if len(filtered_stats) > 0:
                x_pos = range(len(filtered_stats))
                bars = ax.bar(x_pos, filtered_stats['winrate'], 
                             color=['green' if wr > 0.5 else 'red' for wr in filtered_stats['winrate']],
                             alpha=0.7)
                
                ax.set_xlabel('Choice Value')
                ax.set_ylabel('Winrate')
                ax.set_title(f'{choice_name} - Winrate per Choice (≥10 samples)')
                ax.set_xticks(x_pos)
                ax.set_xticklabels([str(int(idx)) for idx in filtered_stats.index], rotation=45)
                ax.grid(True, alpha=0.3)
                
                # Add value labels on bars
                for bar, winrate in zip(bars, filtered_stats['winrate']):
                    if winrate > 0:
                        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{winrate:.3f}', ha='center', va='bottom', fontsize=8)
            else:
                ax.text(0.5, 0.5, f'No choices with ≥10 samples\nfor {choice_name}', 
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{choice_name} - Insufficient data')
        
        # Hide empty subplots
        for i in range(n_choices, n_rows * n_cols):
            row = i // n_cols
            col = i % n_cols
            if n_rows > 1:
                axes[row, col].set_visible(False)
            else:
                axes[col].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/choice_winrates.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # Create a summary heatmap showing best choices per dimension
        print(f"Creating choice summary heatmap...")
        
        # Prepare data for heatmap
        summary_data = []
        choice_names = []
        
        for choice_name, winrate_stats in self.choice_winrate_data.items():
            # Get choices with at least 20 samples for more reliable statistics
            reliable_choices = winrate_stats[winrate_stats['total_nodes'] >= 20]
            if len(reliable_choices) > 0:
                choice_names.append(choice_name)
                # Get top 10 choices or all if less than 10
                top_choices = reliable_choices.head(10)
                choice_winrates = []
                choice_labels = []
                
                for choice_val, row in top_choices.iterrows():
                    choice_winrates.append(row['winrate'])
                    choice_labels.append(f"{int(choice_val)}")
                
                # Pad with zeros if less than 10 choices
                while len(choice_winrates) < 10:
                    choice_winrates.append(0)
                    choice_labels.append("")
                
                summary_data.append(choice_winrates)
        
        if summary_data:
            plt.figure(figsize=(12, max(6, len(choice_names) * 0.5)))
            
            summary_array = np.array(summary_data)
            mask = summary_array == 0
            
            sns.heatmap(summary_array, 
                       yticklabels=choice_names,
                       xticklabels=[f"Top {i+1}" for i in range(10)],
                       annot=True, fmt='.3f', cmap='RdYlGn', 
                       mask=mask, cbar_kws={'label': 'Winrate'})
            
            plt.title('Top Choice Winrates by Dimension (≥20 samples)')
            plt.xlabel('Choice Rank (by winrate)')
            plt.ylabel('Choice Dimension')
            plt.tight_layout()
            plt.savefig(f"{output_dir}/choice_summary_heatmap.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"Choice winrate visualizations saved:")
            print(f"  - {output_dir}/choice_winrates.png")
            print(f"  - {output_dir}/choice_summary_heatmap.png")
    
    def analyze_depth_patterns(self):
        """Analyze patterns related to search depth."""
        print("\n" + "="*60)
        print("DEPTH PATTERN ANALYSIS")
        print("="*60)
        
        # Depth distribution
        depth_counts = self.combined_data['depth'].value_counts().sort_index()
        print(f"\nDepth distribution (top 10 most common depths):")
        for depth, count in depth_counts.head(10).items():
            pct = count / len(self.combined_data) * 100
            print(f"  Depth {depth}: {count:,} nodes ({pct:.2f}%)")
        
        # Winning probability by depth
        print(f"\nWinning probability by depth (first 20 depths):")
        for depth in sorted(self.combined_data['depth'].unique())[:20]:
            depth_data = self.combined_data[self.combined_data['depth'] == depth]
            if len(depth_data) > 0:
                win_prob = len(depth_data[depth_data['on_winning_path'] == 1]) / len(depth_data)
                print(f"  Depth {depth}: {win_prob:.3f} ({len(depth_data):,} nodes)")
        
        # Combinations vs depth
        print(f"\nAverage combinations available by depth (first 15 depths):")
        for depth in sorted(self.combined_data['depth'].unique())[:15]:
            depth_data = self.combined_data[self.combined_data['depth'] == depth]
            if len(depth_data) > 0:
                avg_comb = depth_data['next_combinations'].mean()
                print(f"  Depth {depth}: {avg_comb:,.0f} avg combinations")
    
    def analyze_branching_factor(self):
        """Analyze branching factor patterns."""
        print("\n" + "="*60)
        print("BRANCHING FACTOR ANALYSIS")
        print("="*60)
        
        # Distribution of next_combinations
        combinations = self.combined_data['next_combinations']
        non_zero_combinations = combinations[combinations > 0]
        
        print(f"Branching factor statistics:")
        print(f"  Nodes with 0 combinations (leaves): {len(combinations[combinations == 0]):,}")
        print(f"  Nodes with >0 combinations: {len(non_zero_combinations):,}")
        
        if len(non_zero_combinations) > 0:
            print(f"  Min combinations: {non_zero_combinations.min():,}")
            print(f"  Max combinations: {non_zero_combinations.max():,}")
            print(f"  Median combinations: {non_zero_combinations.median():,}")
            print(f"  Mean combinations: {non_zero_combinations.mean():,.0f}")
            
            # Percentiles
            percentiles = [10, 25, 50, 75, 90, 95, 99]
            print(f"  Percentiles:")
            for p in percentiles:
                val = np.percentile(non_zero_combinations, p)
                print(f"    {p}th percentile: {val:,.0f}")
    
    def generate_visualizations(self, output_dir="analysis_plots"):
        """Generate visualizations of the data."""
        print(f"\n" + "="*60)
        print("GENERATING VISUALIZATIONS")
        print("="*60)
        
        # Create output directory
        Path(output_dir).mkdir(exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # 1. Depth distribution
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        depth_counts = self.combined_data['depth'].value_counts().sort_index()
        plt.bar(depth_counts.index[:30], depth_counts.values[:30])
        plt.xlabel('Depth')
        plt.ylabel('Number of Nodes')
        plt.title('Node Distribution by Depth (First 30 depths)')
        plt.xticks(rotation=45)
        
        plt.subplot(1, 2, 2)
        winning_by_depth = self.combined_data.groupby('depth')['on_winning_path'].mean()
        plt.plot(winning_by_depth.index[:30], winning_by_depth.values[:30], 'o-')
        plt.xlabel('Depth')
        plt.ylabel('Winning Probability')
        plt.title('Winning Probability by Depth')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/depth_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Combinations distribution
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        combinations = self.combined_data['next_combinations']
        non_zero = combinations[combinations > 0]
        if len(non_zero) > 0:
            plt.hist(np.log10(non_zero), bins=50, alpha=0.7)
            plt.xlabel('Log10(Next Combinations)')
            plt.ylabel('Frequency')
            plt.title('Distribution of Next Combinations (Log Scale)')
        
        plt.subplot(1, 2, 2)
        # Combinations vs winning probability
        # Bin combinations and calculate winning probability for each bin
        combinations_binned = pd.cut(combinations[combinations > 0], bins=20)
        win_prob_by_comb = self.combined_data[self.combined_data['next_combinations'] > 0].groupby(
            pd.cut(self.combined_data[self.combined_data['next_combinations'] > 0]['next_combinations'], bins=20)
        )['on_winning_path'].mean()
        
        if len(win_prob_by_comb) > 0:
            x_pos = range(len(win_prob_by_comb))
            plt.bar(x_pos, win_prob_by_comb.values)
            plt.xlabel('Combination Bins')
            plt.ylabel('Winning Probability')
            plt.title('Winning Probability vs Next Combinations')
            plt.xticks(x_pos[::2], [f"{int(interval.left/1000)}K" for interval in win_prob_by_comb.index[::2]], rotation=45)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/combinations_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Per-round comparison
        if len(self.data) > 1:
            plt.figure(figsize=(15, 8))
            
            plt.subplot(2, 2, 1)
            round_stats = []
            for round_num in sorted(self.data.keys()):
                df = self.data[round_num]
                stats = {
                    'round': round_num,
                    'total_nodes': len(df),
                    'winning_nodes': len(df[df['on_winning_path'] == 1]),
                    'max_depth': df['depth'].max(),
                    'avg_combinations': df['next_combinations'].mean()
                }
                round_stats.append(stats)
            
            round_df = pd.DataFrame(round_stats)
            
            plt.bar(round_df['round'], round_df['total_nodes'])
            plt.xlabel('Round')
            plt.ylabel('Total Nodes')
            plt.title('Total Nodes Explored per Round')
            
            plt.subplot(2, 2, 2)
            win_rates = round_df['winning_nodes'] / round_df['total_nodes']
            plt.bar(round_df['round'], win_rates)
            plt.xlabel('Round')
            plt.ylabel('Winning Rate')
            plt.title('Winning Rate per Round')
            
            plt.subplot(2, 2, 3)
            plt.bar(round_df['round'], round_df['max_depth'])
            plt.xlabel('Round')
            plt.ylabel('Max Depth')
            plt.title('Maximum Depth per Round')
            
            plt.subplot(2, 2, 4)
            plt.bar(round_df['round'], round_df['avg_combinations'])
            plt.xlabel('Round')
            plt.ylabel('Avg Combinations')
            plt.title('Average Combinations per Round')
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/round_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. Choice patterns heatmap (if we have choice data)
        if self.choice_columns and len(self.choice_columns) > 1:
            plt.figure(figsize=(12, 8))
            
            # Create correlation matrix of choices
            choice_data = self.combined_data[self.choice_columns].fillna(0)
            if choice_data.shape[1] > 1:
                correlation_matrix = choice_data.corr()
                
                mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
                sns.heatmap(correlation_matrix, mask=mask, annot=True, 
                           cmap='coolwarm', center=0, fmt='.3f',
                           square=True, cbar_kws={"shrink": .8})
                plt.title('Choice Correlation Matrix')
                plt.tight_layout()
                plt.savefig(f"{output_dir}/choice_correlations.png", dpi=300, bbox_inches='tight')
                plt.close()
        
        print(f"Visualizations saved to {output_dir}/ directory:")
        print(f"  - depth_analysis.png")
        print(f"  - combinations_analysis.png")
        if len(self.data) > 1:
            print(f"  - round_comparison.png")
        if self.choice_columns and len(self.choice_columns) > 1:
            print(f"  - choice_correlations.png")
    
    def find_interesting_patterns(self):
        """Find interesting patterns and insights in the data."""
        print("\n" + "="*60)
        print("INTERESTING PATTERNS & INSIGHTS")
        print("="*60)
        
        # 1. Most successful choice combinations
        if self.choice_columns:
            print("\n1. Most successful choice combinations:")
            winning_data = self.combined_data[self.combined_data['on_winning_path'] == 1]
            
            if len(winning_data) > 0:
                # Find most common choice combinations in winning paths
                choice_combinations = winning_data[self.choice_columns].fillna(-1).astype(int)
                combination_counts = choice_combinations.value_counts().head(5)
                
                print("   Top 5 winning choice combinations:")
                for i, (combination, count) in enumerate(combination_counts.items(), 1):
                    print(f"   {i}. {dict(zip(self.choice_columns, combination))} - {count} occurrences")
        
        # 2. Depth efficiency analysis
        print("\n2. Depth efficiency analysis:")
        depth_efficiency = self.combined_data.groupby('depth').agg({
            'on_winning_path': ['count', 'sum', 'mean'],
            'next_combinations': 'mean'
        }).round(3)
        
        depth_efficiency.columns = ['total_nodes', 'winning_nodes', 'win_rate', 'avg_combinations']
        most_efficient_depths = depth_efficiency[depth_efficiency['total_nodes'] >= 10].nlargest(5, 'win_rate')
        
        print("   Most efficient depths (min 10 nodes):")
        for depth, row in most_efficient_depths.iterrows():
            print(f"   Depth {depth}: {row['win_rate']:.3f} win rate "
                  f"({row['winning_nodes']:.0f}/{row['total_nodes']:.0f} nodes)")
        
        # 3. Combination sweet spots
        print("\n3. Combination 'sweet spots':")
        # Find ranges of next_combinations that have high winning rates
        combinations = self.combined_data[self.combined_data['next_combinations'] > 0]['next_combinations']
        if len(combinations) > 0:
            # Create bins and analyze winning rates
            n_bins = min(20, len(combinations.unique()))
            if n_bins > 1:
                binned_data = self.combined_data[self.combined_data['next_combinations'] > 0].copy()
                binned_data['comb_bin'] = pd.cut(binned_data['next_combinations'], bins=n_bins)
                
                bin_analysis = binned_data.groupby('comb_bin').agg({
                    'on_winning_path': ['count', 'sum', 'mean'],
                    'next_combinations': ['min', 'max']
                }).round(3)
                
                bin_analysis.columns = ['total', 'winning', 'win_rate', 'min_comb', 'max_comb']
                best_bins = bin_analysis[bin_analysis['total'] >= 5].nlargest(3, 'win_rate')
                
                print("   Best combination ranges (min 5 nodes):")
                for bin_range, row in best_bins.iterrows():
                    print(f"   {row['min_comb']:.0f}-{row['max_comb']:.0f} combinations: "
                          f"{row['win_rate']:.3f} win rate ({row['winning']:.0f}/{row['total']:.0f} nodes)")
        
        # 4. Round consistency analysis
        if len(self.data) > 1:
            print("\n4. Round consistency analysis:")
            round_win_rates = {}
            for round_num, df in self.data.items():
                win_rate = len(df[df['on_winning_path'] == 1]) / len(df)
                round_win_rates[round_num] = win_rate
            
            win_rate_std = np.std(list(round_win_rates.values()))
            print(f"   Win rate standard deviation across rounds: {win_rate_std:.4f}")
            
            if win_rate_std < 0.01:
                print("   → Very consistent winning rates across rounds")
            elif win_rate_std < 0.05:
                print("   → Moderately consistent winning rates across rounds")
            else:
                print("   → High variation in winning rates across rounds")
                
            print("   Individual round win rates:")
            for round_num in sorted(round_win_rates.keys()):
                print(f"   Round {round_num}: {round_win_rates[round_num]:.4f}")
    
    def export_summary_report(self, filename="dfs_exploration_summary.txt"):
        """Export a comprehensive summary report."""
        print(f"\n" + "="*60)
        print("EXPORTING SUMMARY REPORT")
        print("="*60)
        
        with open(filename, 'w') as f:
            f.write("DFS EXPLORATION ANALYSIS SUMMARY REPORT\n")
            f.write("="*50 + "\n\n")
            
            # Basic stats
            total_nodes = len(self.combined_data)
            winning_nodes = len(self.combined_data[self.combined_data['on_winning_path'] == 1])
            
            f.write("BASIC STATISTICS:\n")
            f.write(f"Total nodes explored: {total_nodes:,}\n")
            f.write(f"Nodes on winning paths: {winning_nodes:,} ({winning_nodes/total_nodes*100:.2f}%)\n")
            f.write(f"Dead end nodes: {total_nodes-winning_nodes:,} ({(total_nodes-winning_nodes)/total_nodes*100:.2f}%)\n")
            f.write(f"Number of rounds: {len(self.data)}\n")
            f.write(f"Choice dimensions: {len(self.choice_columns)}\n\n")
            
            # Depth stats
            f.write("DEPTH ANALYSIS:\n")
            f.write(f"Max depth reached: {self.combined_data['depth'].max()}\n")
            f.write(f"Average depth: {self.combined_data['depth'].mean():.2f}\n")
            f.write(f"Median depth: {self.combined_data['depth'].median():.2f}\n\n")
            
            # Combination stats
            f.write("BRANCHING FACTOR ANALYSIS:\n")
            combinations = self.combined_data['next_combinations']
            non_zero = combinations[combinations > 0]
            f.write(f"Max combinations: {combinations.max():,}\n")
            f.write(f"Average combinations (non-zero): {non_zero.mean():,.0f}\n")
            f.write(f"Median combinations (non-zero): {non_zero.median():,.0f}\n\n")
            
            # Per-round summary
            f.write("PER-ROUND SUMMARY:\n")
            for round_num in sorted(self.data.keys()):
                df = self.data[round_num]
                total = len(df)
                winning = len(df[df['on_winning_path'] == 1])
                f.write(f"Round {round_num}: {total:,} nodes, {winning:,} winning ({winning/total*100:.2f}%)\n")
        
        print(f"Summary report exported to: {filename}")
    
    def run_full_analysis(self, output_dir="analysis_output"):
        """Run complete analysis pipeline."""
        print("Starting DFS Exploration Analysis...")
        print("="*60)
        
        # Load data
        self.load_data()
        
        # Run all analyses
        self.basic_statistics()
        self.analyze_choice_patterns()
        self.analyze_winrate_per_choice()
        self.analyze_depth_patterns()
        self.analyze_branching_factor()
        self.find_interesting_patterns()
        
        # Generate visualizations
        plot_dir = f"{output_dir}/plots"
        self.generate_visualizations(plot_dir)
        self.generate_choice_winrate_plots(plot_dir)
        
        # Export data
        Path(output_dir).mkdir(exist_ok=True)
        self.export_choice_winrates(f"{output_dir}/choice_winrates.csv")
        
        # Export summary
        summary_file = f"{output_dir}/summary_report.txt"
        self.export_summary_report(summary_file)
        
        print(f"\n" + "="*60)
        print("ANALYSIS COMPLETE")
        print("="*60)
        print(f"Results saved to: {output_dir}/")
        print(f"  - Plots: {plot_dir}/")
        print(f"  - Choice winrates CSV: {output_dir}/choice_winrates.csv")
        print(f"  - Summary: {summary_file}")

def main():
    parser = argparse.ArgumentParser(description="Analyze DFS exploration CSV data")
    parser.add_argument("--pattern", default="dfs_exploration_round_*.csv",
                       help="Pattern to match CSV files (default: dfs_exploration_round_*.csv)")
    parser.add_argument("--output", default="analysis_output",
                       help="Output directory for results (default: analysis_output)")
    parser.add_argument("--no-plots", action="store_true",
                       help="Skip generating plots")
    
    args = parser.parse_args()
    
    try:
        analyzer = DFSExplorationAnalyzer(args.pattern)
        
        if args.no_plots:
            analyzer.load_data()
            analyzer.basic_statistics()
            analyzer.analyze_choice_patterns()
            analyzer.analyze_depth_patterns()
            analyzer.analyze_branching_factor()
            analyzer.find_interesting_patterns()
            analyzer.export_summary_report(f"{args.output}/summary_report.txt")
        else:
            analyzer.run_full_analysis(args.output)
            
    except Exception as e:
        print(f"Error during analysis: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())